{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNghPWA2Q3So"
      },
      "outputs": [],
      "source": [
        "#FINAL\n",
        "\n",
        "# Import all necessary libraries\n",
        "import webbrowser     # For opening web pages\n",
        "import time\n",
        "import re\n",
        "import requests       # For http request\n",
        "\n",
        "# Define the starting(seed URL) and current URL\n",
        "seed_url=\"https://en.wikipedia.org\"\n",
        "current_url=\"https://en.wikipedia.org/wiki/Ancient_Indian_sciences\"\n",
        "\n",
        "# Function for  downloading web page\n",
        "def download_page(url):\n",
        "  try:\n",
        "    response=requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return  response.text\n",
        "  except Exception as e:\n",
        "    print(\"ERROR downloading page!!:\"+ url +\"\\n\" +str(e))\n",
        "    print(\"Either the Wikipedia page does not exist or there is an ERROR in the spelling entered! Try again:\")\n",
        "    return None\n",
        "\n",
        "# Fuction for parsing URLs\n",
        "def url_parse(url):\n",
        "  try:\n",
        "    from urllib.parse import urlparse\n",
        "  except ImportError:\n",
        "    from urlparse import urlparse\n",
        "\n",
        "  url=url\n",
        "  s=urlparse(url)\n",
        "  seed_url_n=seed_url\n",
        "  i=0\n",
        "  flag=0\n",
        "\n",
        "  while i<=9:\n",
        "    if url==\"/\":\n",
        "      url=seed_url_n\n",
        "      flag=0\n",
        "    elif not s.scheme:\n",
        "      url=\"http://\" +url\n",
        "      flag=0\n",
        "    elif \"#\" in url:\n",
        "      url=url[:url.find(\"#\")]\n",
        "      flag=0\n",
        "    elif \"?\" in url:\n",
        "      url=url[:url.find(\"?\")]\n",
        "      flag=0\n",
        "    elif s.netloc==\"\":\n",
        "      url=seed_url+s.path\n",
        "      flag=0\n",
        "    elif url[len(url)-1]==\"/\":\n",
        "      url=url[:-1]\n",
        "      flag=0\n",
        "    else:\n",
        "      url=url\n",
        "      flag=0\n",
        "      break\n",
        "\n",
        "    i=i+1\n",
        "    s=urlparse(url)\n",
        "  return(url,flag)\n",
        "\n",
        "# Function for opening Wikipedia page Search Results\n",
        "def open_search_results(keyword):\n",
        "  search_url=\"https://en.wikipedia.org/w/index.php?search=\"+ keyword.replace(' ','+')\n",
        "  webbrowser.open(search_url)\n",
        "\n",
        "# Function for extracting title from raw HTML content\n",
        "def extract_title(page):\n",
        "  start_title=page.find(\"<title>\")\n",
        "  end_title=page.find(\"</title>\",start_title)\n",
        "  if start_title!=-1 and end_title!=-1:\n",
        "    title=page[start_title + 7:end_title]\n",
        "    return title\n",
        "  return \"No Title\"\n",
        "\n",
        "# Function for extracting introduction from raw HTML content\n",
        "def extract_introduction(page):\n",
        "  start_introduction=page.find(\"<p>\")\n",
        "  stop_introduction=page.find('<div id=\"toctitle\">',start_introduction +1)\n",
        "  if '<div id=\"toctitle\">' not in page:\n",
        "    stop_introduction=page.find('</p>',start_introduction +1)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  raw_introduction=page[start_introduction: stop_introduction]\n",
        "  return raw_introduction\n",
        "\n",
        "# Function for extracting title from raw HTML content\n",
        "def extract_links(page):\n",
        "  links=[]\n",
        "  while True:\n",
        "    start_link=page.find(\"<a href\")\n",
        "    if start_link==-1:\n",
        "      break\n",
        "    else:\n",
        "      start_quote=page.find('\"',start_link)\n",
        "      end_quote=page.find('\"',start_quote +1)\n",
        "      link=page[start_quote +1:end_quote]\n",
        "      links.append(link)\n",
        "      page=page[end_quote:]\n",
        "  return links\n",
        "\n",
        "# Function for removing HTML tags from the text\n",
        "def remove_html_tags(text):\n",
        "  clean_text=re.sub(r'<.+?>','',text)\n",
        "  return clean_text\n",
        "\n",
        "# Function for checking File type\n",
        "def extension_scan(url):\n",
        "  a=['.png','.jpg','.jpeg','.gif','.tif','.txt']\n",
        "  j=0\n",
        "  while j<(len(a)):\n",
        "    if a[j] in url:\n",
        "      flag2=1\n",
        "      break\n",
        "    else:\n",
        "      flag2=0\n",
        "      j=j+1\n",
        "  return flag2\n",
        "\n",
        "# Function for the main Crawler part\n",
        "def web_crawl(start_url,depth=3):\n",
        "  to_crawl=[start_url]\n",
        "  database={}\n",
        "  l=0\n",
        "\n",
        "  for k in range(0, depth):\n",
        "    i=0\n",
        "    crawled=[]\n",
        "    while i<3:\n",
        "      if not to_crawl:\n",
        "        break\n",
        "      url=to_crawl.pop(0)\n",
        "      url, flag=url_parse(url)\n",
        "      # flag=url_parse(url)\n",
        "      flag2=extension_scan(url)\n",
        "      time.sleep(3)\n",
        "\n",
        "      if flag==1 or flag2==1:\n",
        "        pass\n",
        "      else:\n",
        "        l=l+1\n",
        "        if url in crawled:\n",
        "          print(\"link\" +str(l)+ \": \" +url)\n",
        "          print(\"VISITED!!\\n\")\n",
        "        else:\n",
        "          print(\"Link\" +str(l)+ \": \" +url)\n",
        "          raw_html=download_page(url)\n",
        "          if raw_html:\n",
        "            title_upper=extract_title(raw_html)\n",
        "            title=title_upper.lower()\n",
        "            print(\"Title = \"+title)\n",
        "            raw_introduction=extract_introduction(raw_html)\n",
        "            to_crawl=to_crawl+extract_links(raw_introduction)\n",
        "            crawled.append(url)\n",
        "            pure_introduction=remove_html_tags(raw_introduction)\n",
        "            print(\"Introduction = \"+pure_introduction.replace('   ',' '))\n",
        "            database[title]=pure_introduction\n",
        "\n",
        "            file=open('database.txt','a')\n",
        "            file.write(title+ \":\" +\"\\n\")\n",
        "            file.write(pure_introduction+\"\\n\")\n",
        "            file.close()\n",
        "\n",
        "            n=1\n",
        "            j=0\n",
        "            while j< (len(to_crawl)-n):\n",
        "              if to_crawl[j] in to_crawl[j+1: (len(to_crawl)-1)]:\n",
        "                to_crawl.pop(j)\n",
        "                n=n+1\n",
        "              else:\n",
        "                pass\n",
        "              j=j+1\n",
        "\n",
        "        i=i+1\n",
        "        # print(i)\n",
        "        # print(k)\n",
        "  return database\n",
        "\n",
        "# Main loop\n",
        "while True:\n",
        "  keyword=input(\"Press 'Enter-Key' to search the 'Ancient-Indian-Sciences-and-Relics' Wikipedia Page OR\\nEnter a search query (or 'q' to quit):\").strip()\n",
        "  if keyword.lower()=='q':\n",
        "    break\n",
        "  else:\n",
        "    # if not keyword.lower():\n",
        "    if not keyword:\n",
        "      print(\"Crawling the DEFAULT page for this search engine !!\")\n",
        "    else:\n",
        "      current_url=\"https://en.wikipedia.org/wiki/\"+keyword.replace(' ','_')\n",
        "      open_search_results(keyword)\n",
        "      print(\"Wikipedia Crawler started!!\")\n",
        "\n",
        "    t0=time.time()\n",
        "    database=web_crawl(start_url=current_url,depth=3)\n",
        "    t1=time.time()\n",
        "    total_time=t1-t0\n",
        "    print(\"Wikipedia Crawler Ended!!\")\n",
        "    print(\"Total Time taken for the exceution(Time Complexity):\",total_time)\n",
        "\n",
        "    link_stack=[]\n",
        "    while True:\n",
        "      print(\"\\n\"*2)\n",
        "      action=input(\"Entera link number to view details (INTEGER!!) or enter 'q' to quit:\").strip()\n",
        "      if action.lower()=='q':\n",
        "        break\n",
        "      elif action.isdigit() and int(action)<=len(database):\n",
        "        title=list(database.keys())[int(action)-1]\n",
        "        print(\"Title:\",title)\n",
        "        print(\"Introduction:\",database[title])\n",
        "        link_stack.append(list(database.keys())[int(action)-1])\n",
        "        selected_link=link_stack.pop()\n",
        "        current_url=\"https://en.wikipedia.org/wiki/\"+ selected_link.replace(' ','_').replace('_-_wikipedia','')\n",
        "        print(\"Wikipedia Crawler Started!!\")\n",
        "        open_search_results(selected_link)\n",
        "\n",
        "        t0=time.time()\n",
        "        database=web_crawl(start_url=current_url,depth=3)\n",
        "        t1=time.time()\n",
        "        total_time=t1-t0\n",
        "        print(\"Wikipedia Crawler Ended!!\")\n",
        "        print(\"Total Time taken for the exceution(Time Complexity):\",total_time)\n",
        "        print(\"\\n\"*2)\n",
        "      else:\n",
        "        break"
      ]
    }
  ]
}